{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/lam.nguyen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lam.nguyen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lam.nguyen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from helper import *\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import CountVectorizerModel, IDFModel\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from pyspark.ml.clustering import LocalLDAModel, LDA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise SparkSpession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/lam.nguyen/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/lam.nguyen/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/lam.nguyen/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-74cda493-7f09-4624-97c0-5db67cec334b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;6.0.0 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.jsoup#jsoup;1.18.2 in central\n",
      "\tfound jakarta.mail#jakarta.mail-api;2.1.3 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;2.1.3 in central\n",
      "\tfound org.eclipse.angus#angus-mail;2.0.3 in central\n",
      "\tfound org.eclipse.angus#angus-activation;2.0.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml;4.1.2 in central\n",
      "\tfound org.apache.poi#poi;4.1.2 in central\n",
      "\tfound org.apache.commons#commons-collections4;4.4 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound com.zaxxer#SparseBitSet;1.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml-schemas;4.1.2 in central\n",
      "\tfound org.apache.xmlbeans#xmlbeans;3.1.0 in central\n",
      "\tfound org.apache.commons#commons-compress;1.19 in central\n",
      "\tfound com.github.virtuald#curvesapi;1.06 in central\n",
      "\tfound org.apache.poi#poi-scratchpad;4.1.2 in central\n",
      "\tfound org.apache.pdfbox#pdfbox;2.0.28 in central\n",
      "\tfound org.apache.pdfbox#fontbox;2.0.28 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.19.2 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.6 in central\n",
      "\tfound org.jetbrains#annotations;24.1.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 in central\n",
      ":: resolution report :: resolve 5077ms :: artifacts dl 161ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.github.virtuald#curvesapi;1.06 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;6.0.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.19.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcom.zaxxer#SparseBitSet;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;2.1.3 from central in [default]\n",
      "\tjakarta.mail#jakarta.mail-api;2.1.3 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.commons#commons-collections4;4.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.19 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.pdfbox#fontbox;2.0.28 from central in [default]\n",
      "\torg.apache.pdfbox#pdfbox;2.0.28 from central in [default]\n",
      "\torg.apache.poi#poi;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml-schemas;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-scratchpad;4.1.2 from central in [default]\n",
      "\torg.apache.xmlbeans#xmlbeans;3.1.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.eclipse.angus#angus-activation;2.0.2 from central in [default]\n",
      "\torg.eclipse.angus#angus-mail;2.0.3 from central in [default]\n",
      "\torg.jetbrains#annotations;24.1.0 from central in [default]\n",
      "\torg.jsoup#jsoup;1.18.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\tcommons-codec#commons-codec;1.13 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  104  |   0   |   0   |   6   ||   98  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-74cda493-7f09-4624-97c0-5db67cec334b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 98 already retrieved (0kB/41ms)\n",
      "25/05/05 23:21:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  6.0.0\n",
      "Apache Spark version:  3.5.5\n"
     ]
    }
   ],
   "source": [
    "# If run sparknlp, use this\n",
    "sparknlp_session = sparknlp.start(params={\"spark.driver.host\": \"localhost\",\n",
    "                                          \"spark.driver.port\": \"9999\",\n",
    "                                          \"spark.driver.bindAddress\": \"127.0.0.1\",\n",
    "                                          \"spark.driver.maxResultSize\": \"2g\"})\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", sparknlp_session.version)\n",
    "# sparknlp_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/05 23:19:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# # If run PySpark, use this\n",
    "# spark = SparkSession.builder\\\n",
    "#         .appName(\"Nhom09_PySparkLDA\")\\\n",
    "#         .master(\"local[*]\")\\\n",
    "#         .config(\"spark.driver.bindAddress\", \"localhost\")\\\n",
    "#         .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_json(os.path.join(\"..\", \"data\", \"merged\", \"corpus.json\"))\n",
    "# corpus_ps = sparknlp_session.createDataFrame(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a pyspark Pipeline, TF and IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 22:25:21 WARN StopWordsCleaner: Default locale set was [en_VN]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 22:26:39 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "25/05/04 22:26:39 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/lam.nguyen/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.5.5.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/05/04 22:26:51 WARN TaskSetManager: Stage 2 contains a task of very large size (52730 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/04 22:38:38 WARN TaskSetManager: Stage 3 contains a task of very large size (52730 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/04 22:38:40 WARN TaskSetManager: Stage 4 contains a task of very large size (52730 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/04 22:38:50 WARN TaskSetManager: Stage 8 contains a task of very large size (52730 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train_pipeline, train_tf, train_idf = pipeline_model(train_data=corpus.copy(deep=True),    \n",
    "#                                                      spark_session=sparknlp_session)\n",
    "# train_pipeline.save(os.path.join(\"..\", \"model\", \"pipeline\"))\n",
    "# train_tf.save(os.path.join(\"..\", \"model\", \"tf\"))\n",
    "# train_idf.save(os.path.join(\"..\", \"model\", \"idf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/lam.nguyen/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.5.5.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/05/05 23:22:16 WARN StopWordsCleaner: Default locale set was [en_VN]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "25/05/05 23:22:19 WARN StopWordsCleaner: Default locale set was [en_VN]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_path = os.path.join(\"..\", \"model\", \"pipeline\")\n",
    "tf_path = os.path.join(\"..\", \"model\", \"tf\")\n",
    "idf_path = os.path.join(\"..\", \"model\", \"idf\")\n",
    "\n",
    "pipeline = PipelineModel.load(pipeline_path)\n",
    "tf = CountVectorizerModel.load(tf_path)\n",
    "idf = IDFModel.load(idf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess using nltk\n",
    "# corpus_nltk, tfidf_nltk = preprocess_nltk(data=corpus.copy(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (using SparkNLP)\n",
    "tfidf_ps = preprocess_sparknlp(data=corpus.copy(deep=True), # PySpark DataFrame\n",
    "                               spark_session=sparknlp_session,\n",
    "                               tfidf_model=(tf, idf),\n",
    "                               pipeline_model=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract features (using nltk)\n",
    "# tfidf_nltk = pd.read_json(os.path.join(\"..\", \"data\", \"processed\", \"features_nltk.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTLK preprocessing vs SparkNLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_rows = [row for row in range(1000, 11000, 1000)]\n",
    "# num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 12:31:19 WARN StopWordsCleaner: Default locale set was [en_VN]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "25/05/04 12:31:21 WARN StopWordsCleaner: Default locale set was [en_VN]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    }
   ],
   "source": [
    "# # Load SparkNLP trained PipelineModel\n",
    "# pipeline_path = os.path.join(\"..\", \"model\", \"pipeline\")\n",
    "# tf_path = os.path.join(\"..\", \"model\", \"tf\")\n",
    "# idf_path = os.path.join(\"..\", \"model\", \"idf\")\n",
    "\n",
    "# pipeline = PipelineModel.load(pipeline_path)\n",
    "# tf = CountVectorizerModel.load(tf_path)\n",
    "# idf = IDFModel.load(idf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_time = []\n",
    "# sparknlp_time = []\n",
    "# for row in num_rows:\n",
    "#     subset_df = corpus.copy(deep=True).head(row)\n",
    "#     # NLTK time\n",
    "#     nltk_start = time.time()\n",
    "#     nlkt_result = preprocess_nltk(data=subset_df)\n",
    "#     nltk_end = time.time()\n",
    "#     nltk_time.append(nltk_end - nltk_start)\n",
    "\n",
    "#     # SparkNLP time\n",
    "#     sparknlp_start = time.time()\n",
    "#     sparknlp_result = preprocess_sparknlp(data=subset_df,\n",
    "#                                           spark_session=sparknlp_session,\n",
    "#                                           tfidf_model=(tf, idf),\n",
    "#                                           pipeline_model=pipeline)\n",
    "#     sparknlp_end = time.time()\n",
    "#     sparknlp_time.append(sparknlp_end - sparknlp_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nltk</th>\n",
       "      <th>sparknlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>296</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>347</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>408</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>456</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>510</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>642</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nltk  sparknlp\n",
       "0    63        13\n",
       "1   115        22\n",
       "2   174        33\n",
       "3   230        48\n",
       "4   296        57\n",
       "5   347        67\n",
       "6   408        79\n",
       "7   456        87\n",
       "8   510       130\n",
       "9   642       116"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time_df = pd.read_csv(os.path.join(\"..\", \"evaluation_result\", \"nltk_sparknlp.csv\"))\n",
    "# time_df[['nltk', 'sparknlp']] = time_df[['nltk', 'sparknlp']].astype(int)\n",
    "# time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Plotting \n",
    "# paper_size = 0.02561 # 10000 papers has a total of 256,1MB\n",
    "# horizontal_axis = [num_papers*paper_size for num_papers in num_rows]\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.set_palette(sns.color_palette(\"muted\"))\n",
    "# sns.set_style('darkgrid')\n",
    "# plt.plot(horizontal_axis, time_df['nltk'], label='NLTK', marker='o')\n",
    "# plt.plot(horizontal_axis, time_df['sparknlp'], label='SparkNLP', marker='o')\n",
    "# plt.xlabel('Data size (MB)')\n",
    "# plt.ylabel('Time (seconds)')\n",
    "# plt.title('NLTK vs SparkNLP')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# # plt.savefig(os.path.join(\"..\", \"img\", 'nltk_sparknlp.png'), format='png', dpi=500)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput, time, speedup when use PySpark vs Scikit-learn to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_rows = [row for row in range(1000, 11000, 1000)]\n",
    "# num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_topics = 5\n",
    "# max_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_df = [pd.read_csv(os.path.join(\"..\", \"data\", \"throughput_cal\", str(row) + \".csv\"), index_col=0) for row in num_rows]\n",
    "# subset_ps = []\n",
    "# for row in num_rows:\n",
    "#     path = os.path.join(\"..\", \"data\", \"throughput_cal\", str(row) + \".csv\")\n",
    "#     tmp_df = pd.read_csv(os.path.join(\"..\", \"data\", \"throughput_cal\", str(row) + \".csv\"))\n",
    "\n",
    "#     row_objects = [\n",
    "#         Row(tf_idf_features=Vectors.dense(vec)) for vec in tmp_df.values\n",
    "#     ]\n",
    "#     tfidf = spark.createDataFrame(row_objects)\n",
    "#     subset_ps.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_df = [tfidf_nltk.copy(deep=True).head(row) for row in num_rows]\n",
    "# subset_ps = []\n",
    "# for df in subset_df:\n",
    "#     tmp = [\n",
    "#         Row(tf_idf_features=Vectors.dense(vec)) for vec in df.values\n",
    "#     ]\n",
    "#     tfidf = sparknlp_session.createDataFrame(tmp)\n",
    "#     subset_ps.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3352016,\n",
       " 6704016,\n",
       " 10056016,\n",
       " 13408016,\n",
       " 16760016,\n",
       " 20112016,\n",
       " 23464016,\n",
       " 26816016,\n",
       " 30168016,\n",
       " 33520016]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sys\n",
    "# input_size = [sys.getsizeof(df) for df in subset_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 is complete\n",
      "pyspark: 1.4593920707702637 - sklearn 5.064054012298584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 08:38:26 WARN TaskSetManager: Stage 242 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:26 WARN TaskSetManager: Stage 243 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:26 WARN TaskSetManager: Stage 244 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:26 WARN TaskSetManager: Stage 245 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:26 WARN TaskSetManager: Stage 246 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:26 WARN TaskSetManager: Stage 247 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:26 WARN TaskSetManager: Stage 248 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:26 WARN TaskSetManager: Stage 249 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 250 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 251 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 252 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 253 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 254 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 255 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 256 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 257 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 258 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 259 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 260 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 261 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 262 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:27 WARN TaskSetManager: Stage 263 contains a task of very large size (1856 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2000 is complete\n",
      "pyspark: 1.7062091827392578 - sklearn 9.010257005691528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 08:38:41 WARN TaskSetManager: Stage 264 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:42 WARN TaskSetManager: Stage 265 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:42 WARN TaskSetManager: Stage 266 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:42 WARN TaskSetManager: Stage 267 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:42 WARN TaskSetManager: Stage 268 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:42 WARN TaskSetManager: Stage 269 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:42 WARN TaskSetManager: Stage 270 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:42 WARN TaskSetManager: Stage 271 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:42 WARN TaskSetManager: Stage 272 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:42 WARN TaskSetManager: Stage 273 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 274 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 275 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 276 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 277 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 278 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 279 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 280 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 281 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 282 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 283 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 284 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:38:43 WARN TaskSetManager: Stage 285 contains a task of very large size (2780 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3000 is complete\n",
      "pyspark: 1.8465158939361572 - sklearn 14.066035985946655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 08:39:02 WARN TaskSetManager: Stage 286 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:02 WARN TaskSetManager: Stage 287 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:02 WARN TaskSetManager: Stage 288 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:02 WARN TaskSetManager: Stage 289 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:02 WARN TaskSetManager: Stage 290 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 291 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 292 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 293 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 294 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 295 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 296 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 297 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 298 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 299 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 300 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 301 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 302 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 303 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 304 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:03 WARN TaskSetManager: Stage 305 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:04 WARN TaskSetManager: Stage 306 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:04 WARN TaskSetManager: Stage 307 contains a task of very large size (3704 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 4000 is complete\n",
      "pyspark: 2.153738021850586 - sklearn 18.35744571685791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 08:39:28 WARN TaskSetManager: Stage 308 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 309 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 310 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 311 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 312 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 313 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 314 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 315 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 316 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 317 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 318 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:29 WARN TaskSetManager: Stage 319 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:30 WARN TaskSetManager: Stage 320 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:30 WARN TaskSetManager: Stage 321 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:30 WARN TaskSetManager: Stage 322 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:30 WARN TaskSetManager: Stage 323 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:30 WARN TaskSetManager: Stage 324 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:30 WARN TaskSetManager: Stage 325 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:30 WARN TaskSetManager: Stage 326 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:30 WARN TaskSetManager: Stage 327 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:30 WARN TaskSetManager: Stage 328 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:39:30 WARN TaskSetManager: Stage 329 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5000 is complete\n",
      "pyspark: 2.686095952987671 - sklearn 23.97981071472168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 08:40:00 WARN TaskSetManager: Stage 330 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:01 WARN TaskSetManager: Stage 331 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:01 WARN TaskSetManager: Stage 332 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:01 WARN TaskSetManager: Stage 333 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:01 WARN TaskSetManager: Stage 334 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:01 WARN TaskSetManager: Stage 335 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:01 WARN TaskSetManager: Stage 336 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:01 WARN TaskSetManager: Stage 337 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:02 WARN TaskSetManager: Stage 338 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:02 WARN TaskSetManager: Stage 339 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:02 WARN TaskSetManager: Stage 340 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:02 WARN TaskSetManager: Stage 341 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:02 WARN TaskSetManager: Stage 342 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:02 WARN TaskSetManager: Stage 343 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:02 WARN TaskSetManager: Stage 344 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:02 WARN TaskSetManager: Stage 345 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:02 WARN TaskSetManager: Stage 346 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:02 WARN TaskSetManager: Stage 347 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:03 WARN TaskSetManager: Stage 348 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:03 WARN TaskSetManager: Stage 349 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:03 WARN TaskSetManager: Stage 350 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:03 WARN TaskSetManager: Stage 351 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 6000 is complete\n",
      "pyspark: 3.050767183303833 - sklearn 29.715411901474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 08:40:35 WARN TaskSetManager: Stage 352 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:36 WARN TaskSetManager: Stage 353 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:36 WARN TaskSetManager: Stage 354 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:36 WARN TaskSetManager: Stage 355 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:36 WARN TaskSetManager: Stage 356 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:36 WARN TaskSetManager: Stage 357 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:37 WARN TaskSetManager: Stage 358 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:37 WARN TaskSetManager: Stage 359 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:37 WARN TaskSetManager: Stage 360 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:37 WARN TaskSetManager: Stage 361 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:37 WARN TaskSetManager: Stage 362 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:37 WARN TaskSetManager: Stage 363 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:37 WARN TaskSetManager: Stage 364 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:37 WARN TaskSetManager: Stage 365 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:37 WARN TaskSetManager: Stage 366 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:37 WARN TaskSetManager: Stage 367 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:38 WARN TaskSetManager: Stage 368 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:38 WARN TaskSetManager: Stage 369 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:38 WARN TaskSetManager: Stage 370 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:38 WARN TaskSetManager: Stage 371 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:38 WARN TaskSetManager: Stage 372 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:40:38 WARN TaskSetManager: Stage 373 contains a task of very large size (3793 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 7000 is complete\n",
      "pyspark: 3.5251970291137695 - sklearn 32.06306195259094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 08:41:19 WARN TaskSetManager: Stage 374 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:21 WARN TaskSetManager: Stage 375 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:21 WARN TaskSetManager: Stage 376 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:21 WARN TaskSetManager: Stage 377 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:21 WARN TaskSetManager: Stage 378 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:22 WARN TaskSetManager: Stage 379 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:22 WARN TaskSetManager: Stage 380 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:23 WARN TaskSetManager: Stage 381 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:23 WARN TaskSetManager: Stage 382 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:23 WARN TaskSetManager: Stage 383 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:23 WARN TaskSetManager: Stage 384 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:23 WARN TaskSetManager: Stage 385 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:24 WARN TaskSetManager: Stage 386 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:24 WARN TaskSetManager: Stage 387 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:24 WARN TaskSetManager: Stage 388 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:24 WARN TaskSetManager: Stage 389 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:25 WARN TaskSetManager: Stage 390 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:25 WARN TaskSetManager: Stage 391 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:25 WARN TaskSetManager: Stage 392 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:25 WARN TaskSetManager: Stage 393 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:26 WARN TaskSetManager: Stage 394 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:41:26 WARN TaskSetManager: Stage 395 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 8000 is complete\n",
      "pyspark: 6.760176181793213 - sklearn 40.583986043930054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 08:42:10 WARN TaskSetManager: Stage 396 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:11 WARN TaskSetManager: Stage 397 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:11 WARN TaskSetManager: Stage 398 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:11 WARN TaskSetManager: Stage 399 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:11 WARN TaskSetManager: Stage 400 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:11 WARN TaskSetManager: Stage 401 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:11 WARN TaskSetManager: Stage 402 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:11 WARN TaskSetManager: Stage 403 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:11 WARN TaskSetManager: Stage 404 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:11 WARN TaskSetManager: Stage 405 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 406 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 407 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 408 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 409 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 410 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 411 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 412 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 413 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 414 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 415 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 416 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:42:12 WARN TaskSetManager: Stage 417 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 9000 is complete\n",
      "pyspark: 2.7346110343933105 - sklearn 43.801127910614014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 08:43:07 WARN TaskSetManager: Stage 418 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:08 WARN TaskSetManager: Stage 419 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:08 WARN TaskSetManager: Stage 420 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:08 WARN TaskSetManager: Stage 421 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:08 WARN TaskSetManager: Stage 422 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:08 WARN TaskSetManager: Stage 423 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:09 WARN TaskSetManager: Stage 424 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:09 WARN TaskSetManager: Stage 425 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:09 WARN TaskSetManager: Stage 426 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:09 WARN TaskSetManager: Stage 427 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:09 WARN TaskSetManager: Stage 428 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:09 WARN TaskSetManager: Stage 429 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:10 WARN TaskSetManager: Stage 430 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:10 WARN TaskSetManager: Stage 431 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:10 WARN TaskSetManager: Stage 432 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:10 WARN TaskSetManager: Stage 433 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:10 WARN TaskSetManager: Stage 434 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:10 WARN TaskSetManager: Stage 435 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:10 WARN TaskSetManager: Stage 436 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:10 WARN TaskSetManager: Stage 437 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:10 WARN TaskSetManager: Stage 438 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 08:43:10 WARN TaskSetManager: Stage 439 contains a task of very large size (7577 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10000 is complete\n",
      "pyspark: 3.731799840927124 - sklearn 54.37480092048645\n"
     ]
    }
   ],
   "source": [
    "# sklearn_time = []\n",
    "# pyspark_time = []\n",
    "# for i in range(len(num_rows)):\n",
    "#     # scikit-learn time\n",
    "#     sklearn_start = time.time()\n",
    "#     sklearn_model = LatentDirichletAllocation(n_components=num_topics, \n",
    "#                                               max_iter=max_iter)\n",
    "#     sklearn_model.fit(subset_df[i])\n",
    "#     sklearn_end = time.time()\n",
    "#     sklearn_time.append(sklearn_end - sklearn_start)\n",
    "\n",
    "#     # pyspark time\n",
    "#     pyspark_start = time.time()\n",
    "#     pyspark_model = LDA(k=num_topics, \n",
    "#                         maxIter=max_iter, \n",
    "#                         featuresCol='tf_idf_features')\n",
    "#     pyspark_model.fit(subset_ps[i])\n",
    "#     subset_ps[i].unpersist()\n",
    "#     pyspark_end = time.time()\n",
    "#     pyspark_time.append(pyspark_end - pyspark_start)\n",
    "\n",
    "#     print(f\"First {num_rows[i]} is complete\")\n",
    "#     print(f\"pyspark: {pyspark_end - pyspark_start} - sklearn {sklearn_end - sklearn_start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size = round(sys.getsizeof(tfidf_nltk.iloc[:,0])/(1024**2),2) #size of one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Plotting \n",
    "# throughput_sklearn = [round(num_rows[i]/sklearn_time[i]*sample_size,2) for i in range(len(num_rows))]\n",
    "# throughput_pyspark = [round(num_rows[i]/pyspark_time[i]*sample_size,2) for i in range(len(num_rows))]\n",
    "# horizontal_axis = [round(size/(1024**2),2) for size in input_size]\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.set_palette(sns.color_palette(\"muted\"))\n",
    "# sns.set_style('darkgrid')\n",
    "# plt.plot(horizontal_axis, throughput_sklearn, label='Sklearn', marker='o')\n",
    "# plt.plot(horizontal_axis, throughput_pyspark, label='PySpark', marker='o')\n",
    "# plt.xlabel('Data size (MB)')\n",
    "# plt.ylabel('Throughput (MB/sec)')\n",
    "# plt.title('Throughput of Sklearn vs PySpark')\n",
    "# plt.xticks([int(size) for size in horizontal_axis])\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(os.path.join(\"..\", \"img\", 'sklearn_pyspark_throughput.png'), format='png', dpi=500)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28818836349484334,\n",
       " 0.1893874765398396,\n",
       " 0.1312679524004924,\n",
       " 0.11730500017947522,\n",
       " 0.11202635914332784,\n",
       " 0.10267448561065727,\n",
       " 0.10995098368360974,\n",
       " 0.16658216438510506,\n",
       " 0.06243036846476391,\n",
       " 0.06864037815648713]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# speedup = [throughput_sklearn[i]/throughput_pyspark[i] for i in range(len(throughput_sklearn))]\n",
    "# speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ks/tcmmkdsn54j_lc0yv7dzh4sc0000gp/T/ipykernel_3116/203693749.py:13: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Plotting \n",
    "# speedup = [throughput_pyspark[i] / throughput_sklearn[i] for i in range(len(throughput_sklearn))]\n",
    "# horizontal_axis = [round(size/(1024**2),2) for size in input_size]\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.set_palette(sns.color_palette(\"muted\"))\n",
    "# sns.set_style('darkgrid')\n",
    "# plt.plot(horizontal_axis, speedup, marker='o')\n",
    "# plt.xlabel('Data size (MB)')\n",
    "# plt.ylabel('Speedup')\n",
    "# plt.title('Speedup of Sklearn vs PySpark')\n",
    "# plt.xticks([int(size) for size in horizontal_axis])\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(os.path.join(\"..\", \"img\", 'sklearn_pyspark_speedup.png'), format='png', dpi=500)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm performance: Coherence (PMI) and Diversity (PUW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "max_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model = LDA(k=num_topics, \n",
    "#                     maxIter=max_iter, \n",
    "#                     featuresCol='tf_idf_features')\n",
    "# lda = lda_model.fit(tfidf_ps)\n",
    "# # lda.save(os.path.join(\"..\", \"model\", \"lda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LocalLDAModel.load(os.path.join(\"..\",\"model\", \"lda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_profile = lda.transform(tfidf_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topic=0, termIndices=[16, 63, 0, 120, 1, 99, 283, 90, 65, 235]),\n",
       " Row(topic=1, termIndices=[2, 3, 86, 173, 57, 221, 16, 136, 209, 90]),\n",
       " Row(topic=2, termIndices=[4, 3, 2, 29, 56, 71, 72, 74, 114, 48]),\n",
       " Row(topic=3, termIndices=[2, 3, 98, 114, 131, 44, 11, 32, 56, 58]),\n",
       " Row(topic=4, termIndices=[0, 25, 1, 9, 108, 43, 175, 51, 128, 125]),\n",
       " Row(topic=5, termIndices=[11, 82, 157, 31, 214, 132, 266, 143, 58, 14]),\n",
       " Row(topic=6, termIndices=[23, 25, 72, 129, 56, 9, 4, 27, 188, 74]),\n",
       " Row(topic=7, termIndices=[29, 84, 40, 131, 25, 44, 32, 82, 56, 52]),\n",
       " Row(topic=8, termIndices=[11, 153, 238, 76, 66, 222, 311, 118, 319, 289]),\n",
       " Row(topic=9, termIndices=[86, 218, 19, 136, 46, 44, 1, 82, 16, 166])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_indices = lda.describeTopics(10).select(\"topic\", \"termIndices\").collect()\n",
    "term_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 23:23:51 WARN TaskSetManager: Stage 29 contains a task of very large size (52730 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/05 23:32:26 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 388984 ms exceeds timeout 120000 ms\n",
      "25/05/05 23:32:26 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "25/05/05 23:32:32 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:32:32 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:32:42 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:32:42 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:32:52 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:32:52 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:02 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:02 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:12 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:12 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:22 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:22 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:32 ERROR Inbox: Ignoring error                       (0 + 4) / 4]\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:32 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:42 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:42 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:52 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:33:52 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:02 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:02 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:12 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:12 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:22 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:22 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:32 ERROR Inbox: Ignoring error                       (0 + 4) / 4]\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:32 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:42 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:42 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:52 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:34:52 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:02 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:02 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:12 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:12 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:22 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:22 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:32 ERROR Inbox: Ignoring error                       (0 + 4) / 4]\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:32 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:42 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:42 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:52 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:35:52 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:36:02 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:36:02 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:36:12 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:36:12 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:36:22 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:36:22 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:36:32 ERROR Inbox: Ignoring error                       (0 + 4) / 4]\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:36:32 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:36:42 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "25/05/05 23:36:42 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "25/05/05 23:36:52 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "25/05/05 23:36:52 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "25/05/05 23:37:02 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:02 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:12 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:12 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:22 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:22 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:32 WARN Executor: Issue communicating with driver in heartbeater]\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:32 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:42 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:42 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:52 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:37:52 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:38:02 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:38:02 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:38:12 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:38:12 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:38:22 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:38:22 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lam.nguyen/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/lam.nguyen/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[Stage 29:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus_profile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 23:38:32 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/05/05 23:38:32 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:9999\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "tokens = corpus_profile.select('finish').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_tmp = tf.transform(pipeline.transform(sparknlp_session.createDataFrame(corpus.copy(deep=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lam.nguyen/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/lam.nguyen/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)): \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinish\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary\u001b[49m:\n\u001b[1;32m      9\u001b[0m             word_frequency[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;66;03m# word_frequency_documents[word].add(i)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/pyspark/ml/feature.py:1228\u001b[0m, in \u001b[0;36mCountVectorizerModel.vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.6.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvocabulary\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m    An array of terms in the vocabulary.\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_java\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocabulary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/pyspark/ml/wrapper.py:72\u001b[0m, in \u001b[0;36mJavaWrapper._call_java\u001b[0;34m(self, name, *args)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_java2py\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/pyspark/ml/common.py:115\u001b[0m, in \u001b[0;36m_java2py\u001b[0;34m(sc, r, encoding)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, (JavaArray, JavaList)):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLSerDe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError:\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# not picklable\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculate PMI\n",
    "## Get word_frequency and word_frequency_documents\n",
    "word_frequency = {word: 0 for word in tf.vocabulary}\n",
    "word_frequency_documents = {word: set() for word in tf.vocabulary}\n",
    "num_docs = len(corpus)\n",
    "for i in range(len(tokens)): \n",
    "    for word in tokens[i]['finish']:\n",
    "        if word in tf.vocabulary:\n",
    "            word_frequency[word] += 1\n",
    "            # word_frequency_documents[word].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(word_frequency).to_csv(os.path.join(\"..\", \"word_frequency.csv\"))\n",
    "pd.DataFrame(word_frequency_documents).to_csv(os.path.join(\"..\", \"word_frequency_documents.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def pmi(\n",
    "        topic_words: list[str],\n",
    "        word_frequency: dict[int],\n",
    "        word_frequency_documents: dict[str, list],\n",
    "        n_docs: int\n",
    ") -> float:\n",
    "    \"\"\"Calculates PMI for a topic\n",
    "    \n",
    "    Args:\n",
    "        topic_words: word representation of a topic\n",
    "        word_frequency: frequency of each word in corpus\n",
    "        word_frequency_documents: frequency of each word for each document\n",
    "        n_docs: number of documents\n",
    "        \n",
    "    Returns:\n",
    "        The PMI metric for the topic\"\"\"\n",
    "    num_topwords = len(topic_words)\n",
    "    pmi = 0.0\n",
    "\n",
    "    for j in range(1, num_topwords):\n",
    "        for i in range(0, j):\n",
    "            ti = topic_words[i]\n",
    "            tj = topic_words[j]\n",
    "\n",
    "            freq_i = word_frequency[ti]\n",
    "            freq_j = word_frequency[tj]\n",
    "            freq_ij = len(word_frequency_documents[ti].intersection(word_frequency_documents[tj]))\n",
    "\n",
    "            wij_prob = freq_ij / float(num_docs)\n",
    "            wi_wj_prob = ((freq_i*freq_j) / float(num_docs) **2)\n",
    "\n",
    "            pmi += np.log(wij_prob / wi_wj_prob)\n",
    "\n",
    "    return pmi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
