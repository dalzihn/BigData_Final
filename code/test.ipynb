{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lam.nguyen/Desktop/GithubClone/BigData_Final/.venv/lib/python3.10/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/lam.nguyen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lam.nguyen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lam.nguyen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_files() -> None:\n",
    "#     \"\"\"Merge all JSON files of a folder\n",
    "\n",
    "#     Sample folder structure\n",
    "#     ```\n",
    "#     â””â”€â”€ ðŸ“data\n",
    "#         â””â”€â”€ ðŸ“merged\n",
    "#             â””â”€â”€ test.json\n",
    "#         â””â”€â”€ ðŸ“processed\n",
    "#         â””â”€â”€ ðŸ“raw\n",
    "#             â””â”€â”€ test01.json\n",
    "#             â””â”€â”€ test02.json\n",
    "#     ```\n",
    "#     Args:\n",
    "#         None\n",
    "#     Return:\n",
    "#         returns None\"\"\"\n",
    "#     files = os.listdir(os.path.join(\"..\", \"data\", \"raw\"))\n",
    "#     text_todf = []\n",
    "#     for i in range(len(files)):\n",
    "#         with open(os.path.join(\"..\", \"data\", \"raw\", files[i]), 'r') as file:\n",
    "#             data = json.load(file)\n",
    "#             df_metadata = pd.json_normalize(data, max_level=1)\n",
    "#             text = \"\"\n",
    "#             for element in df_metadata['body_text'][0]:\n",
    "#                 text += element['text']\n",
    "#             text_todf.append(text)\n",
    "#     df = pd.DataFrame({\"text\": text_todf})\n",
    "#     df.to_json(os.path.join(\"..\", \"data\", \"merged\", \"corpus.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_json(os.path.join(\"..\", \"data\", \"merged\", \"test.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_word_tokenize(sent_tokens: list[str]) -> list[str]:\n",
    "    \"\"\"Tokenises words \n",
    "    \n",
    "    Args:\n",
    "        sent_tokens: tokens of sentences\n",
    "    Returns:\n",
    "        A list of word tokens\"\"\"\n",
    "    word_tokens = []\n",
    "    for sent in sent_tokens:\n",
    "        tokens = word_tokenize(sent)\n",
    "        word_tokens.extend(tokens)\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Performs the preprocessing\n",
    "    \n",
    "    Args:\n",
    "        data: input file as pandas.DataFrame\n",
    "    Returns:\n",
    "        A pandas.DataFrame which as vector form of the input\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "    #Sentence tokenisation\n",
    "    data['sent_tokens'] = data['text'].apply(sent_tokenize)\n",
    "    \n",
    "    # Text cleaning\n",
    "    data['sent_tokens'] = data['sent_tokens'].apply(lambda sentences: [re.sub(r\"[^a-zA-Z\\s]\", \"\", sent) for sent in sentences])\n",
    "    \n",
    "    # # Normalisation\n",
    "    data['sent_tokens'] = data['sent_tokens'].apply(lambda sentences: [sent.lower() for sent in sentences])\n",
    "\n",
    "    #Word tokenisation\n",
    "    data['word_tokens'] = data['sent_tokens'].apply(custom_word_tokenize)\n",
    "\n",
    "    # # Stemming\n",
    "    data['word_tokens'] = data['word_tokens'].apply(lambda word_tokens: [lemmatiser.lemmatize(word) for word in word_tokens])\n",
    "\n",
    "    # Word Embeddings (turns into vector)\n",
    "    data['to_tfidf'] = data['word_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "    vectorizer = TfidfVectorizer(min_df=0.3, max_df=0.85, stop_words=stop_words)\n",
    "    tfidf_matrix = vectorizer.fit_transform(data['to_tfidf'])\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return data, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, tfidf  = preprocess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/28 08:49:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create spark spession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Nhom09_PySparkLDA\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .config(\"spark.driver.bindAddress\", \"localhost\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit tfidf (panads.DataFrame) into data structure of spark\n",
    "tfidf_ps = spark.createDataFrame(tfidf)\n",
    "tfidf_ps = tfidf_ps.rdd.map(lambda row: (Vectors.dense(row), )).toDF([\"features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+---------------------------------------------------------------------+\n",
      "|topic|termIndices    |termWeights                                                          |\n",
      "+-----+---------------+---------------------------------------------------------------------+\n",
      "|0    |[404, 41, 105] |[0.002398466433135447, 0.002334772230784771, 0.0023063777294193063]  |\n",
      "|1    |[343, 164, 35] |[0.0023859780701145855, 0.0023294478680108786, 0.0023203198160067742]|\n",
      "|2    |[108, 8, 232]  |[0.0024689895728190795, 0.002377024731400353, 0.0023564226010567954] |\n",
      "|3    |[176, 220, 273]|[0.0023427878938911107, 0.0023394471897149522, 0.0023343600894544132]|\n",
      "|4    |[26, 196, 278] |[0.0024366442857787266, 0.002359836168195985, 0.002356088139912443]  |\n",
      "+-----+---------------+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Create an LDA model\n",
    "lda = LDA(k=5, maxIter=10)  # k is the number of topics\n",
    "\n",
    "# Fit the LDA model\n",
    "lda_model = lda.fit(tfidf_ps)\n",
    "# Describe topics\n",
    "topics = lda_model.describeTopics(3)  # Get the top 3 terms for each topic\n",
    "topics.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
