{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9015c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\diept\\miniconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\diept\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f4f46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba078d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\diept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\diept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\diept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5530f08c",
   "metadata": {},
   "source": [
    "# Preprocess funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4814928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files() -> None:\n",
    "#     \"\"\"Merge all JSON files of a folder\n",
    "\n",
    "#     Sample folder structure\n",
    "#     ```\n",
    "#     ‚îî‚îÄ‚îÄ üìÅdata\n",
    "#         ‚îî‚îÄ‚îÄ üìÅmerged\n",
    "#             ‚îî‚îÄ‚îÄ test.json\n",
    "#         ‚îî‚îÄ‚îÄ üìÅprocessed\n",
    "#         ‚îî‚îÄ‚îÄ üìÅraw\n",
    "#             ‚îî‚îÄ‚îÄ test01.json\n",
    "#             ‚îî‚îÄ‚îÄ test02.json\n",
    "#     ```\n",
    "#     Args:\n",
    "#         None\n",
    "#     Return:\n",
    "#         returns None\"\"\"\n",
    "    files = os.listdir(os.path.join(\"..\", \"data\", \"raw\"))\n",
    "    text_todf = []\n",
    "    for i in range(len(files)):\n",
    "        with open(os.path.join(\"..\", \"data\", \"raw\", files[i]), 'r') as file:\n",
    "            data = json.load(file)\n",
    "            df_metadata = pd.json_normalize(data, max_level=1)\n",
    "            text = \"\"\n",
    "            for element in df_metadata['body_text'][0]:\n",
    "                text += element['text']\n",
    "            text_todf.append(text)\n",
    "    df = pd.DataFrame({\"text\": text_todf})\n",
    "    df.to_json(os.path.join(\"..\", \"data\", \"merged\", \"corpus.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9017010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a consequence of the global COVID-19 pandem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>According to current live statistics at the ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  As a consequence of the global COVID-19 pandem...\n",
       "1  According to current live statistics at the ti..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Read data\n",
    "data = pd.read_json(os.path.join(\"..\", \"merged\", \"test.json\"))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37be5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data : pd.DataFrame) -> pd.DataFrame : \n",
    "  \"\"\" Performing data preprocessing\n",
    "  Agrs : \n",
    "    data : input file as pandas.DataFrame\n",
    "  Returns :\n",
    "    A pandas.DataFrame which is as vector form of input\"\"\"\n",
    "\n",
    "  #Create a copy file so that we wont change the original data\n",
    "  cleaned_data = data.copy()\n",
    "  \n",
    "\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  def clean_text(text):\n",
    "     #text cleaning, normalization\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text).strip()\n",
    "    #Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    #Text lemmatization, stopword removal\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "  cleaned_data['text_data'] = cleaned_data['text'].apply(clean_text)\n",
    "  \n",
    "\n",
    "  #Rare words, common words filtering\n",
    "  vectorizer = TfidfVectorizer(max_df= 0.9, min_df= 0.2)\n",
    "  #vecorization using TF-IDF\n",
    "  tfidf_data = vectorizer.fit_transform(cleaned_data['text_data'])\n",
    "  #Convert to Dataframe\n",
    "  cleaned_data = pd.DataFrame(tfidf_data.toarray(),columns = vectorizer.get_feature_names_out())\n",
    "  return cleaned_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af5d0b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>academy</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>access</th>\n",
       "      <th>accordance</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accurate</th>\n",
       "      <th>...</th>\n",
       "      <th>widely</th>\n",
       "      <th>window</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>worldwide</th>\n",
       "      <th>would</th>\n",
       "      <th>writing</th>\n",
       "      <th>wtft</th>\n",
       "      <th>wtfts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.057213</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.057213</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.114426</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114426</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.228852</td>\n",
       "      <td>0.028606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.082619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 542 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ability      able   academy  acceleration    access  accordance  \\\n",
       "0  0.057213  0.028606  0.028606      0.028606  0.057213    0.028606   \n",
       "1  0.000000  0.000000  0.000000      0.000000  0.000000    0.000000   \n",
       "\n",
       "   according  account  accuracy  accurate  ...    widely    window     work  \\\n",
       "0    0.00000  0.00000  0.114426  0.028606  ...  0.028606  0.028606  0.00000   \n",
       "1    0.04131  0.04131  0.000000  0.000000  ...  0.000000  0.000000  0.04131   \n",
       "\n",
       "   working    world  worldwide     would  writing      wtft     wtfts  \n",
       "0  0.00000  0.00000   0.000000  0.114426  0.00000  0.228852  0.028606  \n",
       "1  0.04131  0.04131   0.082619  0.000000  0.04131  0.000000  0.000000  \n",
       "\n",
       "[2 rows x 542 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = preprocess(data)\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb92082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output csv\n",
    "processed_data.to_csv(os.path.join('..','cleaned_data.csv'),index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202ce31",
   "metadata": {},
   "source": [
    "# Training LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1922a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\diept\\miniconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\diept\\miniconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d99abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.functions import col, udf \n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d162720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Pyspark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LDA Topic Modeling\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17f6b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform pd.DataFrame to pyspark.sql.DataFrame\n",
    "spark_data = spark.createDataFrame(processed_data) \n",
    "\n",
    "#Combine columns into 1 vector column\n",
    "assembler = VectorAssembler(inputCols= spark_data.columns, outputCol= 'features')\n",
    "processed_data_vector = assembler.transform(spark_data).select('features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize number of topics and max interation\n",
    "num_topics = 10\n",
    "max_interation = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73919792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices                                       |termWeights                                                                                                                                                                                                                          |\n",
      "+-----+--------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[257, 53, 531, 94, 185, 393, 526, 166, 316, 232]  |[0.002556330017035642, 0.002344122630907044, 0.002338477109205982, 0.0023318627629827548, 0.0023306558458723687, 0.002326938076085318, 0.002321610646204159, 0.0023038333972799324, 0.0022925350885307713, 0.0022824623165365013]    |\n",
      "|1    |[379, 223, 431, 305, 438, 513, 483, 116, 336, 380]|[0.0023749106584080824, 0.0023714924353265185, 0.0023375585334234252, 0.0023372148203640183, 0.002329421583206499, 0.0023214945365575396, 0.0022941068615512906, 0.002277365156277673, 0.0022654512560116116, 0.002257895389688907]  |\n",
      "|2    |[66, 212, 275, 290, 345, 129, 263, 16, 205, 293]  |[0.0024285201176633083, 0.0023965299406566837, 0.0023608216768448193, 0.0023209504880235344, 0.0023128558148215726, 0.002302474631160048, 0.0022940660530137735, 0.0022595247628232756, 0.0022502983037370563, 0.0022444738981411095]|\n",
      "|3    |[44, 227, 383, 451, 518, 252, 32, 150, 88, 475]   |[0.0025483407355722065, 0.0023693896867022627, 0.0023511145551401826, 0.002331787045164254, 0.0023038380754043675, 0.002301786915046903, 0.0022687854304685525, 0.0022629431849479713, 0.0022618178012402735, 0.0022491593940431927] |\n",
      "|4    |[419, 489, 172, 168, 313, 189, 271, 426, 352, 257]|[0.0023946898040215644, 0.0023713108318956185, 0.002301453702036161, 0.0022966607941200872, 0.002291276791706279, 0.002263626273390117, 0.0022520941106918793, 0.0022496285615722818, 0.002233088142500076, 0.0022284788988503534]   |\n",
      "|5    |[242, 537, 58, 416, 287, 74, 132, 100, 327, 91]   |[0.002455795562087515, 0.0024040077708937417, 0.0023667242751616237, 0.0023598182746874564, 0.002346458382674281, 0.0023224038954152205, 0.0023077811557176663, 0.002302438878542382, 0.0022457321283733244, 0.0022343172841018575]  |\n",
      "|6    |[376, 64, 222, 447, 418, 498, 135, 227, 407, 525] |[0.0025421159926535264, 0.002423872321335759, 0.002369050275122027, 0.0023426401147663904, 0.002324838439065067, 0.002315872141909184, 0.002275230021773627, 0.0022732815409377345, 0.002261543930561104, 0.0022291214673924344]     |\n",
      "|7    |[314, 126, 29, 102, 108, 188, 346, 457, 189, 315] |[0.002426950713183393, 0.002399526613882588, 0.00239421911704696, 0.0023812363770758104, 0.0023387436146957616, 0.0023268363229292143, 0.0023251519233687913, 0.0022868920345322924, 0.0022818145949117614, 0.0022779492010448635]   |\n",
      "|8    |[303, 110, 174, 309, 326, 489, 31, 213, 523, 475] |[0.0024835689072823935, 0.00245739928602368, 0.002401225180082972, 0.0023838656031829734, 0.0023549259557610887, 0.002324224431525924, 0.002301984446122465, 0.002298368933822644, 0.00227913199559053, 0.00224250645324158]         |\n",
      "|9    |[511, 342, 275, 104, 378, 85, 486, 332, 536, 203] |[0.002398959112868689, 0.00239796631525651, 0.0023935368652035745, 0.0023850900903729572, 0.002330765366593918, 0.002307461158576941, 0.002299875999428312, 0.002280166730972033, 0.0022594609700013328, 0.002254934682278521]       |\n",
      "+-----+--------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train LDA\n",
    "lda = LDA(k=num_topics, maxIter=max_interation, featuresCol= 'features')\n",
    "lda_model = lda.fit(processed_data_vector)\n",
    "# Show topics \n",
    "topics = lda_model.describeTopics(maxTermsPerTopic= 10)\n",
    "topics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60436f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
