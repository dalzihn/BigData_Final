{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9015c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\diept\\miniconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\diept\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f4f46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import các thư viện cần thiết\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba078d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\diept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\diept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\diept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5530f08c",
   "metadata": {},
   "source": [
    "# Preprocess funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9017010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a consequence of the global COVID-19 pandem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>According to current live statistics at the ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  As a consequence of the global COVID-19 pandem...\n",
       "1  According to current live statistics at the ti..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Read data\n",
    "data = pd.read_json(os.path.join(\"..\", \"merged\", \"test.json\"))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37be5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data : pd.DataFrame) -> pd.DataFrame : \n",
    "  \"\"\" Performing data preprocessing\n",
    "  Agrs : \n",
    "    data : input file as pandas.DataFrame\n",
    "  Returns :\n",
    "    A pandas.DataFrame which is as vector form of input\"\"\"\n",
    "\n",
    "  #Create a copy file so that we wont change the original data\n",
    "  cleaned_data = data.copy()\n",
    "  \n",
    "#Prepare stopword and lemmatizer\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  def clean_text(text):\n",
    "     #text cleaning, normalization\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '',text)\n",
    "\n",
    "    #Sentence Segmentation\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    #Tokenization\n",
    "    tokens = []\n",
    "    for s in sentences:\n",
    "      tokens = word_tokenize(s)\n",
    "       #Text lemmatization, stopword removal\n",
    "      tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "      tokens.extend(tokens)\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "  cleaned_data['text_data'] = cleaned_data['text'].apply(clean_text)\n",
    "  \n",
    "\n",
    "  #Rare words, common words filtering\n",
    "  vectorizer = TfidfVectorizer(max_df= 0.9, min_df= 0.2)\n",
    "\n",
    "  #vecorization using TF-IDF\n",
    "  tfidf_data = vectorizer.fit_transform(cleaned_data['text_data'])\n",
    "  \n",
    "  #Convert to Dataframe\n",
    "  cleaned_data = pd.DataFrame(tfidf_data.toarray(),columns = vectorizer.get_feature_names_out())\n",
    "  return cleaned_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af5d0b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>academy</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>access</th>\n",
       "      <th>accordance</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accurate</th>\n",
       "      <th>...</th>\n",
       "      <th>widely</th>\n",
       "      <th>window</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>worldwide</th>\n",
       "      <th>would</th>\n",
       "      <th>writing</th>\n",
       "      <th>wtft</th>\n",
       "      <th>wtfts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.058747</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>0.058747</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117495</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234989</td>\n",
       "      <td>0.029374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041885</td>\n",
       "      <td>0.041885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041885</td>\n",
       "      <td>0.041885</td>\n",
       "      <td>0.041885</td>\n",
       "      <td>0.083771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 552 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ability      able   academy  acceleration    access  accordance  \\\n",
       "0  0.058747  0.029374  0.029374      0.029374  0.058747    0.029374   \n",
       "1  0.000000  0.000000  0.000000      0.000000  0.000000    0.000000   \n",
       "\n",
       "   according   account  accuracy  accurate  ...    widely    window      work  \\\n",
       "0   0.000000  0.000000  0.117495  0.029374  ...  0.029374  0.029374  0.000000   \n",
       "1   0.041885  0.041885  0.000000  0.000000  ...  0.000000  0.000000  0.041885   \n",
       "\n",
       "    working     world  worldwide     would   writing      wtft     wtfts  \n",
       "0  0.000000  0.000000   0.000000  0.117495  0.000000  0.234989  0.029374  \n",
       "1  0.041885  0.041885   0.083771  0.000000  0.041885  0.000000  0.000000  \n",
       "\n",
       "[2 rows x 552 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = preprocess(data)\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb92082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output csv\n",
    "processed_data.to_csv(os.path.join('..','cleaned_data.csv'),index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202ce31",
   "metadata": {},
   "source": [
    "# Training LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1922a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\diept\\miniconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\diept\\miniconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d99abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.functions import col, udf \n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d162720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Pyspark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LDA Topic Modeling\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17f6b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform pd.DataFrame to pyspark.sql.DataFrame\n",
    "spark_data = spark.createDataFrame(processed_data) \n",
    "\n",
    "#Combine columns into 1 vector column\n",
    "assembler = VectorAssembler(inputCols= spark_data.columns, outputCol= 'features')\n",
    "processed_data_vector = assembler.transform(spark_data).select('features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fba9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize number of topics and max interation\n",
    "num_topics = 10\n",
    "max_interation = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73919792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices                                       |termWeights                                                                                                                                                                                                                         |\n",
      "+-----+--------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[46, 372, 146, 7, 508, 540, 437, 298, 234, 74]    |[0.0024123256641410368, 0.00234223749490057, 0.002296095174315453, 0.002267717904296153, 0.002251078181320783, 0.002228780589971868, 0.002215128719344768, 0.002202124674873793, 0.0021843446086544597, 0.0021831183549264155]      |\n",
      "|1    |[186, 532, 343, 524, 528, 480, 90, 412, 420, 351] |[0.0024379377546459163, 0.0023746930017593134, 0.0023266359908837666, 0.0022868663993676564, 0.002258492442735555, 0.0022442852271706668, 0.0022333990358092775, 0.0022176611644853304, 0.002201318837879323, 0.0021957453503755027]|\n",
      "|2    |[142, 395, 181, 371, 497, 478, 503, 392, 483, 272]|[0.00236396379872825, 0.002243975623900352, 0.0022311638168906194, 0.002230322770519873, 0.0021962206544550874, 0.0021894453636809104, 0.002186396670851679, 0.002179999854474497, 0.002165599138960684, 0.002162859734280985]      |\n",
      "|3    |[142, 184, 424, 68, 415, 40, 178, 213, 408, 185]  |[0.0024832137971815623, 0.002328725904059252, 0.0023103415138766587, 0.002291797180496541, 0.0022573189375634435, 0.00223906599235779, 0.002238393430827794, 0.0022281624089704405, 0.0022244787728434683, 0.0022227171260817257]   |\n",
      "|4    |[509, 67, 192, 421, 45, 148, 355, 215, 480, 271]  |[0.0023740199873839244, 0.002355751719822409, 0.0022963646856548146, 0.00227097993166973, 0.002255563997195769, 0.0022476334987362456, 0.002236930490744619, 0.002196536321606449, 0.002185589724504584, 0.0021788798851586793]     |\n",
      "|5    |[108, 457, 175, 189, 225, 170, 44, 463, 162, 488] |[0.002301674080275103, 0.0022644989326845353, 0.002232895431922579, 0.002214217383615885, 0.0022136963126742765, 0.002203806319821133, 0.0022002879694124717, 0.0021890983940667896, 0.0021882564404998345, 0.0021825806078209662]  |\n",
      "|6    |[470, 528, 518, 494, 171, 322, 489, 197, 154, 143]|[0.002300586468007125, 0.00223919723066087, 0.0022231974880615375, 0.0021939242991018864, 0.0021928230961459844, 0.002189315883943746, 0.002179318808224356, 0.002169244989947523, 0.0021675214721440196, 0.002165673863845475]     |\n",
      "|7    |[414, 152, 389, 477, 316, 134, 372, 123, 407, 305]|[0.002394542862662015, 0.002325504753582876, 0.002300679595600611, 0.0022545427451219004, 0.002253036742346265, 0.0022259160433956643, 0.0022238712180927957, 0.0022167847049988513, 0.002207687721964096, 0.0021971732814533617]   |\n",
      "|8    |[69, 120, 188, 389, 78, 141, 504, 276, 61, 333]   |[0.002349876734696899, 0.0022795412584660723, 0.0022767424005390165, 0.0022751573486238046, 0.002261118389458745, 0.0022588552376437013, 0.002253678940040095, 0.0022531156957731, 0.002239168113802658, 0.0022337647693740762]     |\n",
      "|9    |[37, 55, 538, 253, 40, 451, 401, 437, 21, 346]    |[0.002356891166784894, 0.0023181698790455947, 0.0023067364983471233, 0.002234275932057916, 0.00223193037347749, 0.0022262582370898244, 0.002219561785843308, 0.002215351801699624, 0.0022006231039275424, 0.0021924796109473086]    |\n",
      "+-----+--------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train LDA\n",
    "lda = LDA(k=num_topics, maxIter=max_interation, featuresCol= 'features')\n",
    "lda_model = lda.fit(processed_data_vector)\n",
    "# Show topics \n",
    "topics = lda_model.describeTopics(maxTermsPerTopic= 10)\n",
    "topics.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
