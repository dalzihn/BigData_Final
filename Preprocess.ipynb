{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9015c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\diept\\miniconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\diept\\miniconda3\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\diept\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f4f46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import các thư viện cần thiết\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba078d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\diept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\diept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\diept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5530f08c",
   "metadata": {},
   "source": [
    "# Preprocess funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9017010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a consequence of the global COVID-19 pandem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>According to current live statistics at the ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  As a consequence of the global COVID-19 pandem...\n",
       "1  According to current live statistics at the ti..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Read data\n",
    "data = pd.read_json(os.path.join(\"..\", \"merged\", \"test.json\"))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37be5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data : pd.DataFrame) -> pd.DataFrame : \n",
    "  \"\"\" Performing data preprocessing\n",
    "  Agrs : \n",
    "    data : input file as pandas.DataFrame\n",
    "  Returns :\n",
    "    A pandas.DataFrame which is as vector form of input\"\"\"\n",
    "\n",
    "  #Create a copy file so that we wont change the original data\n",
    "  cleaned_data = data.copy()\n",
    "  \n",
    "#Prepare stopword and lemmatizer\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  def clean_text(text):\n",
    "     #text cleaning, normalization\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text).strip()\n",
    "\n",
    "    #Sentence Segmentation\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    #Tokenization\n",
    "    tokens = []\n",
    "    for s in sentences:\n",
    "      tokens = word_tokenize(s)\n",
    "       #Text lemmatization, stopword removal\n",
    "      tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "      tokens.extend(tokens)\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "  cleaned_data['text_data'] = cleaned_data['text'].apply(clean_text)\n",
    "  \n",
    "\n",
    "  #Rare words, common words filtering\n",
    "  vectorizer = TfidfVectorizer(max_df= 0.9, min_df= 0.2)\n",
    "\n",
    "  #vecorization using TF-IDF\n",
    "  tfidf_data = vectorizer.fit_transform(cleaned_data['text_data'])\n",
    "  \n",
    "  #Convert to Dataframe\n",
    "  cleaned_data = pd.DataFrame(tfidf_data.toarray(),columns = vectorizer.get_feature_names_out())\n",
    "  return cleaned_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af5d0b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>academy</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>access</th>\n",
       "      <th>accordance</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accurate</th>\n",
       "      <th>...</th>\n",
       "      <th>widely</th>\n",
       "      <th>window</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>worldwide</th>\n",
       "      <th>would</th>\n",
       "      <th>writing</th>\n",
       "      <th>wtft</th>\n",
       "      <th>wtfts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.057213</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.057213</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.114426</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114426</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.228852</td>\n",
       "      <td>0.028606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.082619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 542 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ability      able   academy  acceleration    access  accordance  \\\n",
       "0  0.057213  0.028606  0.028606      0.028606  0.057213    0.028606   \n",
       "1  0.000000  0.000000  0.000000      0.000000  0.000000    0.000000   \n",
       "\n",
       "   according  account  accuracy  accurate  ...    widely    window     work  \\\n",
       "0    0.00000  0.00000  0.114426  0.028606  ...  0.028606  0.028606  0.00000   \n",
       "1    0.04131  0.04131  0.000000  0.000000  ...  0.000000  0.000000  0.04131   \n",
       "\n",
       "   working    world  worldwide     would  writing      wtft     wtfts  \n",
       "0  0.00000  0.00000   0.000000  0.114426  0.00000  0.228852  0.028606  \n",
       "1  0.04131  0.04131   0.082619  0.000000  0.04131  0.000000  0.000000  \n",
       "\n",
       "[2 rows x 542 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = preprocess(data)\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb92082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output csv\n",
    "processed_data.to_csv(os.path.join('..','cleaned_data.csv'),index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202ce31",
   "metadata": {},
   "source": [
    "# Training LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1922a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\diept\\miniconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\diept\\miniconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d99abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.functions import col, udf \n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d162720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Pyspark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LDA Topic Modeling\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17f6b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform pd.DataFrame to pyspark.sql.DataFrame\n",
    "spark_data = spark.createDataFrame(processed_data) \n",
    "\n",
    "#Combine columns into 1 vector column\n",
    "assembler = VectorAssembler(inputCols= spark_data.columns, outputCol= 'features')\n",
    "processed_data_vector = assembler.transform(spark_data).select('features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fba9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize number of topics and max interation\n",
    "num_topics = 10\n",
    "max_interation = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73919792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices                                      |termWeights                                                                                                                                                                                                                          |\n",
      "+-----+-------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[46, 372, 146, 7, 540, 508, 437, 74, 298, 234]   |[0.0024572161551489912, 0.0023898063689840763, 0.0023390092144922204, 0.002306279236917576, 0.00229764148071278, 0.0022932267665123855, 0.0022644063748045954, 0.0022395921326049334, 0.00223957063007649, 0.0022214882096519735]    |\n",
      "|1    |[196, 353, 534, 538, 422, 100, 430, 490, 20, 180]|[0.002489945595737017, 0.002344182559632889, 0.002327681781003853, 0.002312266816808566, 0.002284347672222184, 0.002276131637283172, 0.002270981393263368, 0.0022629253371357233, 0.0022347062401708567, 0.0022323076687536655]      |\n",
      "|2    |[162, 0, 415, 201, 391, 517, 498, 412, 523, 292] |[0.00240804321080748, 0.002302502368690028, 0.0022858896220274106, 0.002272838488303403, 0.0022719817505130954, 0.0022370911017714264, 0.0022357782893499822, 0.0022315938624575997, 0.002221721922834214, 0.002208696065029655]     |\n",
      "|3    |[172, 214, 454, 98, 445, 70, 208, 438, 215, 243] |[0.0025189136910791668, 0.002368872121596906, 0.002345175842861242, 0.0023161010122099692, 0.0022901300893557745, 0.0022780573246376197, 0.0022761968625326107, 0.002258207088021026, 0.0022549895135817763, 0.0022516465887232892]  |\n",
      "|4    |[107, 232, 461, 85, 188, 395, 255, 520, 17, 311] |[0.0024077341031570228, 0.002347036738173631, 0.0023210918813188074, 0.0023053356953204283, 0.0022972302598283385, 0.0022862910496616196, 0.002245005539770623, 0.002233817367777788, 0.002229824839565386, 0.0022269594648748744]   |\n",
      "|5    |[7, 158, 507, 225, 239, 275, 220, 94, 513, 212]  |[0.002405531951365694, 0.0023387669438409285, 0.00230099267242839, 0.002268879799467067, 0.002249900811693643, 0.0022493713017173247, 0.0022393219741617568, 0.0022357468751374606, 0.0022243770051247726, 0.002223521478919383]     |\n",
      "|6    |[530, 231, 382, 257, 214, 18, 203, 4, 354, 438]  |[0.002343509863498772, 0.0022338927027344374, 0.002222342550059834, 0.002217695854587409, 0.0022002192067751614, 0.0021984728435165885, 0.0021983438494067573, 0.002193665283906466, 0.0021854296651992286, 0.0021811760103163345]   |\n",
      "|7    |[484, 222, 459, 46, 386, 36, 204, 442, 193, 12]  |[0.0024321804836328104, 0.0023620572275338133, 0.0023368418496952916, 0.0022970501804177408, 0.0022884501325634793, 0.0022806370372727915, 0.002260903261755882, 0.0022588262109300623, 0.0022516283964101567, 0.0022506075541143485]|\n",
      "|8    |[149, 200, 268, 469, 158, 221, 5, 356, 141, 413] |[0.0023959165964356345, 0.0023242030292625272, 0.002321349337284959, 0.002319733297217559, 0.002305419291632289, 0.0023031117740279963, 0.002298889107618392, 0.0022972598268315085, 0.0022830389446093, 0.002277529727080285]       |\n",
      "|9    |[127, 145, 42, 343, 130, 541, 491, 527, 111, 436]|[0.002400909229778953, 0.0023614647950754293, 0.0022793013151303928, 0.0022760040498302023, 0.002273614643372433, 0.0022678366397458865, 0.0022610150748538296, 0.002256726385029037, 0.002241722707829719, 0.0022334271871523766]   |\n",
      "+-----+-------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train LDA\n",
    "lda = LDA(k=num_topics, maxIter=max_interation, featuresCol= 'features')\n",
    "lda_model = lda.fit(processed_data_vector)\n",
    "# Show topics \n",
    "topics = lda_model.describeTopics(maxTermsPerTopic= 10)\n",
    "topics.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
